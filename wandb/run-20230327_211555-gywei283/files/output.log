step 0: train loss 10.9889, val loss 10.9901
iter 0: loss 10.9907, time 46469.08ms, mfu -100.00%
Traceback (most recent call last):
  File "/home/linzihao/PreTrain/PreTrain/train.py", line 298, in <module>
    scaler.scale(loss).backward()
  File "/home/linzihao/anaconda3/envs/transformer/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/linzihao/anaconda3/envs/transformer/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.61 GiB (GPU 0; 39.59 GiB total capacity; 29.63 GiB already allocated; 221.19 MiB free; 37.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF